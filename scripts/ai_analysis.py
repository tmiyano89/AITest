#!/usr/bin/env python3
"""
AIåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ¬ãƒãƒ¼ãƒˆã®è©³ç´°åˆ†æã¨è€ƒå¯Ÿã‚’è¿½åŠ ã™ã‚‹
"""

import json
import os
import sys
from datetime import datetime

def load_metrics(log_dir):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    metrics_file = os.path.join(log_dir, 'detailed_metrics.json')
    if not os.path.exists(metrics_file):
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {metrics_file}")
        return None
    
    with open(metrics_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def generate_ai_analysis(metrics):
    """AIã«ã‚ˆã‚‹è©³ç´°åˆ†æã‚’ç”Ÿæˆ"""
    analysis = {
        "timestamp": datetime.now().isoformat(),
        "overall_analysis": {},
        "method_analysis": {},
        "language_analysis": {},
        "pattern_analysis": {},
        "recommendations": [],
        "hypotheses": []
    }
    
    # å…¨ä½“åˆ†æ
    overall = metrics.get('item_metrics', {}).get('overall', {})
    analysis["overall_analysis"] = {
        "summary": f"å…¨ä½“æ­£è¦åŒ–ã‚¹ã‚³ã‚¢: {overall.get('normalized_score', 0):.3f}",
        "key_metrics": {
            "expected_items": overall.get('expected_items', 0),
            "correct_items": overall.get('correct_items', 0),
            "wrong_items": overall.get('wrong_items', 0),
            "missing_items": overall.get('missing_items', 0),
            "unexpected_items": overall.get('unexpected_items', 0)
        },
        "insights": [
            f"æ­£è§£ç‡: {overall.get('correct_items', 0) / (overall.get('expected_items', 1) or 1) * 100:.1f}%",
            f"éå‰°æŠ½å‡ºç‡: {overall.get('unexpected_items', 0) / (overall.get('expected_items', 1) or 1) * 100:.1f}%",
            f"æ¬ è½ç‡: {overall.get('missing_items', 0) / (overall.get('expected_items', 1) or 1) * 100:.1f}%"
        ]
    }
    
    # æŠ½å‡ºæ–¹æ³•åˆ¥åˆ†æ
    method_scores = metrics.get('grouped_scores', {}).get('by_method', {})
    if method_scores:
        best_method = max(method_scores.items(), key=lambda x: x[1]['normalized_score'])
        worst_method = min(method_scores.items(), key=lambda x: x[1]['normalized_score'])
        
        analysis["method_analysis"] = {
            "best_performer": {
                "method": best_method[0],
                "score": best_method[1]['normalized_score'],
                "characteristics": {
                    "correct_items": best_method[1]['correct_items'],
                    "wrong_items": best_method[1]['wrong_items'],
                    "unexpected_items": best_method[1]['unexpected_items']
                }
            },
            "worst_performer": {
                "method": worst_method[0],
                "score": worst_method[1]['normalized_score'],
                "characteristics": {
                    "correct_items": worst_method[1]['correct_items'],
                    "wrong_items": worst_method[1]['wrong_items'],
                    "unexpected_items": worst_method[1]['unexpected_items']
                }
            },
            "performance_gap": best_method[1]['normalized_score'] - worst_method[1]['normalized_score'],
            "insights": [
                f"{best_method[0]}ãŒæœ€ã‚‚é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹",
                f"{worst_method[0]}ã®æ€§èƒ½æ”¹å–„ãŒå¿…è¦",
                f"æ€§èƒ½å·®ã¯{best_method[1]['normalized_score'] - worst_method[1]['normalized_score']:.3f}"
            ]
        }
    
    # è¨€èªåˆ¥åˆ†æ
    language_scores = metrics.get('grouped_scores', {}).get('by_language', {})
    if language_scores:
        best_lang = max(language_scores.items(), key=lambda x: x[1]['normalized_score'])
        worst_lang = min(language_scores.items(), key=lambda x: x[1]['normalized_score'])
        
        analysis["language_analysis"] = {
            "best_performer": {
                "language": best_lang[0],
                "score": best_lang[1]['normalized_score']
            },
            "worst_performer": {
                "language": worst_lang[0],
                "score": worst_lang[1]['normalized_score']
            },
            "language_gap": best_lang[1]['normalized_score'] - worst_lang[1]['normalized_score'],
            "insights": [
                f"{best_lang[0]}ãŒã‚ˆã‚Šé«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹",
                f"è¨€èªé–“ã®æ€§èƒ½å·®ã¯{best_lang[1]['normalized_score'] - worst_lang[1]['normalized_score']:.3f}",
                "è¨€èªå›ºæœ‰ã®æœ€é©åŒ–ãŒå¿…è¦"
            ]
        }
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥åˆ†æ
    pattern_scores = metrics.get('grouped_scores', {}).get('by_pattern', {})
    if pattern_scores:
        best_pattern = max(pattern_scores.items(), key=lambda x: x[1]['normalized_score'])
        worst_pattern = min(pattern_scores.items(), key=lambda x: x[1]['normalized_score'])
        
        analysis["pattern_analysis"] = {
            "best_performer": {
                "pattern": best_pattern[0],
                "score": best_pattern[1]['normalized_score']
            },
            "worst_performer": {
                "pattern": worst_pattern[0],
                "score": worst_pattern[1]['normalized_score']
            },
            "pattern_gap": best_pattern[1]['normalized_score'] - worst_pattern[1]['normalized_score'],
            "insights": [
                f"{best_pattern[0]}ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæœ€ã‚‚é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹",
                f"{worst_pattern[0]}ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ”¹å–„ãŒå¿…è¦",
                "ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¤‡é›‘ã•ã¨æ€§èƒ½ã®é–¢ä¿‚ã‚’åˆ†æã™ã‚‹å¿…è¦ãŒã‚ã‚‹"
            ]
        }
    
    # æ¨å¥¨äº‹é …
    analysis["recommendations"] = [
        "æœ€è‰¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æŠ½å‡ºæ–¹æ³•ã®ç‰¹æ€§ã‚’ä»–ã®æ–¹æ³•ã«é©ç”¨",
        "è¨€èªé–“ã®æ€§èƒ½å·®ã‚’ç¸®å°ã™ã‚‹ãŸã‚ã®æœ€é©åŒ–",
        "ä½æ€§èƒ½ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ®µéšçš„å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¤œè¨",
        "éå‰°æŠ½å‡ºç‡ã®å‰Šæ¸›ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹å–„",
        "æ¬ è½ç‡ã®å‰Šæ¸›ã®ãŸã‚ã®å‰å‡¦ç†æ”¹å–„"
    ]
    
    # ä»®èª¬
    analysis["hypotheses"] = [
        "æŠ½å‡ºæ–¹æ³•ã®æ€§èƒ½å·®ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®é•ã„ã«ã‚ˆã‚‹",
        "è¨€èªé–“ã®æ€§èƒ½å·®ã¯æ–‡åŒ–çš„ãƒ»è¨€èªçš„ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã®é•ã„ã«ã‚ˆã‚‹",
        "ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¤‡é›‘ã•ãŒæ€§èƒ½ã«å½±éŸ¿ã—ã¦ã„ã‚‹",
        "éå‰°æŠ½å‡ºã¯ãƒ¢ãƒ‡ãƒ«ã®å‰µé€ æ€§ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ã®å•é¡Œ",
        "æ¬ è½ã¯ãƒ¢ãƒ‡ãƒ«ã®ä¿å®ˆæ€§ã¨åŒ…æ‹¬æ€§ã®ãƒãƒ©ãƒ³ã‚¹ã®å•é¡Œ"
    ]
    
    return analysis

def save_analysis(analysis, log_dir):
    """åˆ†æçµæœã‚’ä¿å­˜"""
    analysis_file = os.path.join(log_dir, 'ai_analysis.json')
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    print(f"âœ… AIåˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {analysis_file}")

def main():
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python3 scripts/ai_analysis.py <log_directory>")
        sys.exit(1)
    
    log_dir = sys.argv[1]
    if not os.path.exists(log_dir):
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {log_dir}")
        sys.exit(1)
    
    print(f"ğŸ” AIåˆ†æã‚’é–‹å§‹: {log_dir}")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
    metrics = load_metrics(log_dir)
    if not metrics:
        sys.exit(1)
    
    # AIåˆ†æã‚’ç”Ÿæˆ
    analysis = generate_ai_analysis(metrics)
    
    # åˆ†æçµæœã‚’ä¿å­˜
    save_analysis(analysis, log_dir)
    
    print("âœ… AIåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
