#!/usr/bin/env python3
"""
æ‹¡å¼µå¯èƒ½ãªå®Ÿé¨“å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã€å›æ•°ã€è¨€èªã‚’æŒ‡å®šã—ã¦å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ä¸€ã¤ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ•´ç†ã™ã‚‹
"""

import subprocess
import json
import statistics
import os
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import argparse

class ExperimentConfig:
    """å®Ÿé¨“è¨­å®šã‚¯ãƒ©ã‚¹"""
    def __init__(self, pattern: str, language: str = "ja", runs: int = 1):
        self.pattern = pattern
        self.language = language
        self.runs = runs
    
    def get_experiment_name(self) -> str:
        """å®Ÿé¨“åã‚’ç”Ÿæˆ"""
        return f"{self.pattern}_{self.language}"
    
    def get_method(self) -> str:
        """æŠ½å‡ºæ–¹æ³•ã‚’å–å¾—"""
        return "generable"
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆJSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰"""
        return {
            'pattern': self.pattern,
            'language': self.language,
            'runs': self.runs,
            'experiment_name': self.get_experiment_name(),
            'method': self.get_method()
        }

class ExperimentRunner:
    """å®Ÿé¨“å®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_output_dir: str):
        self.base_output_dir = Path(base_output_dir)
        self.base_output_dir.mkdir(parents=True, exist_ok=True)
        self.results = []
    
    def run_single_experiment(self, config: ExperimentConfig, run_id: int) -> Optional[Dict[str, Any]]:
        """å˜ä¸€ã®å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        print(f"ğŸ”¬ å®Ÿé¨“å®Ÿè¡Œä¸­: {config.get_experiment_name()} (å®Ÿè¡Œ {run_id}/{config.runs})")
        
        # å®Ÿé¨“å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
        cmd = [
            "swift", "run", "AITestApp", 
            "--experiment", f"{config.get_method()}_{config.language}",
            "--test-dir", str(self.base_output_dir),
            "--pattern", config.pattern
        ]
        
        # ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦runNumberã‚’æ¸¡ã™
        env = os.environ.copy()
        env['AITEST_RUN_NUMBER'] = str(run_id)
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300, env=env)
            if result.returncode != 0:
                print(f"âŒ å®Ÿé¨“å¤±æ•—: {result.stderr}")
                return None
            
            return {
                'config': config,
                'run_id': run_id,
                'success': True,
                'stdout': result.stdout,
                'stderr': result.stderr
            }
        except subprocess.TimeoutExpired:
            print(f"â° å®Ÿé¨“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            return None
        except Exception as e:
            print(f"âŒ å®Ÿé¨“ä¾‹å¤–: {e}")
            return None
    
    def run_experiments(self, configs: List[ExperimentConfig]) -> List[Dict[str, Any]]:
        """è¤‡æ•°ã®å®Ÿé¨“è¨­å®šã‚’å®Ÿè¡Œ"""
        all_results = []
        
        # ç·å®Ÿè¡Œå›æ•°ã‚’è¨ˆç®—
        total_runs = sum(config.runs for config in configs)
        completed_runs = 0
        
        for config in configs:
            print(f"\nğŸš€ ãƒ‘ã‚¿ãƒ¼ãƒ³ {config.pattern} ã®å®Ÿé¨“ã‚’é–‹å§‹ ({config.runs}å›å®Ÿè¡Œ)")
            print(f"ğŸ“ å‡ºåŠ›å…ˆ: {self.base_output_dir}")
            
            for run_id in range(1, config.runs + 1):
                # é€²æ—è¡¨ç¤º
                progress = (completed_runs / total_runs) * 100
                print(f"ğŸ“Š é€²æ—: {progress:.1f}% ({completed_runs}/{total_runs})")
                
                result = self.run_single_experiment(config, run_id)
                if result:
                    all_results.append(result)
                    self.results.append(result)
                
                completed_runs += 1
        
        # æœ€çµ‚é€²æ—è¡¨ç¤º
        print(f"ğŸ“Š é€²æ—: 100.0% ({completed_runs}/{total_runs}) - å®Œäº†!")
        
        return all_results
    
    def collect_log_files(self) -> List[Dict[str, Any]]:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†"""
        log_data = []
        
        # ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆæ–°ã—ã„å‘½åè¦å‰‡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
        json_files = list(self.base_output_dir.glob("*_level*_run*.json"))
        print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.base_output_dir}")
        print(f"ğŸ” è¦‹ã¤ã‹ã£ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(json_files)}")
        
        for i, json_file in enumerate(json_files, 1):
            progress = (i / len(json_files)) * 100
            print(f"ğŸ“„ å‡¦ç†ä¸­: {json_file.name} ({progress:.1f}%)")
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    log_data.append({
                        'file': str(json_file),
                        'data': data
                    })
                    print(f"âœ… èª­ã¿è¾¼ã¿æˆåŠŸ: {json_file.name}")
            except Exception as e:
                print(f"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {json_file.name}: {e}")
        
        print(f"ğŸ“Š ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åé›†å®Œäº†: {len(log_data)}/{len(json_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
        return log_data
    
    def generate_statistics(self, log_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """çµ±è¨ˆã‚’è¨ˆç®—"""
        if not log_data:
            return {'error': 'ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“'}
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®çµæœã‚’æ•´ç†
        pattern_results = {}
        
        for log in log_data:
            data = log['data']
            # æ–°ã—ã„æ§‹é€ ã«å¯¾å¿œï¼šå€‹åˆ¥ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            pattern = data.get('experiment_pattern', 'unknown')
            if pattern not in pattern_results:
                pattern_results[pattern] = {
                    'correct': [],
                    'wrong': [],
                    'missing': [],
                    'unexpected': [],
                    'expected': [],
                    'test_cases': []
                }
            
            # expected_fieldsã‹ã‚‰æœŸå¾…é …ç›®æ•°ã‚’è¨ˆç®—
            expected_fields = data.get('expected_fields', [])
            expected_count = len(expected_fields)
            
            # correct, wrong, missing, unexpectedã‚’è¨ˆç®—
            correct_count = sum(1 for field in expected_fields if field.get('status') == 'correct')
            wrong_count = sum(1 for field in expected_fields if field.get('status') == 'wrong')
            missing_count = sum(1 for field in expected_fields if field.get('status') == 'missing')
            unexpected_count = len(data.get('unexpected_fields', []))
            
            pattern_results[pattern]['correct'].append(correct_count)
            pattern_results[pattern]['wrong'].append(wrong_count)
            pattern_results[pattern]['missing'].append(missing_count)
            pattern_results[pattern]['unexpected'].append(unexpected_count)
            pattern_results[pattern]['expected'].append(expected_count)
            pattern_results[pattern]['test_cases'].append(data)
        
        # çµ±è¨ˆã‚’è¨ˆç®—
        stats = {}
        for pattern, results in pattern_results.items():
            if results['correct']:
                stats[pattern] = {
                    'total_test_cases': len(results['test_cases']),
                    'correct': {
                        'total': sum(results['correct']),
                        'mean': statistics.mean(results['correct']),
                        'std': statistics.stdev(results['correct']) if len(results['correct']) > 1 else 0
                    },
                    'wrong': {
                        'total': sum(results['wrong']),
                        'mean': statistics.mean(results['wrong']),
                        'std': statistics.stdev(results['wrong']) if len(results['wrong']) > 1 else 0
                    },
                    'missing': {
                        'total': sum(results['missing']),
                        'mean': statistics.mean(results['missing']),
                        'std': statistics.stdev(results['missing']) if len(results['missing']) > 1 else 0
                    },
                    'unexpected': {
                        'total': sum(results['unexpected']),
                        'mean': statistics.mean(results['unexpected']),
                        'std': statistics.stdev(results['unexpected']) if len(results['unexpected']) > 1 else 0
                    },
                    'expected': {
                        'total': sum(results['expected']),
                        'mean': statistics.mean(results['expected']),
                        'std': statistics.stdev(results['expected']) if len(results['expected']) > 1 else 0
                    }
                }
                
                # æ­£è¦åŒ–ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                total_correct = stats[pattern]['correct']['total']
                total_wrong = stats[pattern]['wrong']['total']
                total_unexpected = stats[pattern]['unexpected']['total']
                total_expected = stats[pattern]['expected']['total']
                
                if total_expected > 0:
                    normalized_score = (total_correct - total_wrong - total_unexpected) / total_expected
                    stats[pattern]['normalized_score'] = normalized_score
                else:
                    stats[pattern]['normalized_score'] = 0
        
        return stats
    
    def generate_report(self, stats: Dict[str, Any]):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("\n" + "="*80)
        print("ğŸ“Š å®Ÿé¨“çµæœãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*80)
        print(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.base_output_dir}")
        print()
        
        if 'error' in stats:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {stats['error']}")
            return
        
        for pattern, data in stats.items():
            print(f"ğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern}")
            print("-" * 40)
            print(f"ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: {data['total_test_cases']}")
            print(f"æ­£è¦åŒ–ã‚¹ã‚³ã‚¢: {data['normalized_score']:.4f}")
            print(f"æ­£è§£é …ç›®æ•°: {data['correct']['total']} (å¹³å‡: {data['correct']['mean']:.1f} Â± {data['correct']['std']:.1f})")
            print(f"èª¤ã‚Šé …ç›®æ•°: {data['wrong']['total']} (å¹³å‡: {data['wrong']['mean']:.1f} Â± {data['wrong']['std']:.1f})")
            print(f"ä¸è¶³é …ç›®æ•°: {data['missing']['total']} (å¹³å‡: {data['missing']['mean']:.1f} Â± {data['missing']['std']:.1f})")
            print(f"ä½™åˆ†é …ç›®æ•°: {data['unexpected']['total']} (å¹³å‡: {data['unexpected']['mean']:.1f} Â± {data['unexpected']['std']:.1f})")
            print(f"æœŸå¾…é …ç›®æ•°: {data['expected']['total']} (å¹³å‡: {data['expected']['mean']:.1f} Â± {data['expected']['std']:.1f})")
            print()
    
    def save_results(self, stats: Dict[str, Any], log_data: List[Dict[str, Any]]):
        """çµæœã‚’ä¿å­˜"""
        output_file = self.base_output_dir / "experiment_results.json"
        
        # ExperimentConfigã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        serializable_results = []
        for result in self.results:
            serializable_result = result.copy()
            if 'config' in serializable_result:
                serializable_result['config'] = serializable_result['config'].to_dict()
            serializable_results.append(serializable_result)
        
        result_data = {
            'experiment_info': {
                'timestamp': datetime.now().isoformat(),
                'output_directory': str(self.base_output_dir),
                'total_experiments': len(self.results)
            },
            'statistics': stats,
            'raw_log_data': log_data,
            'experiment_results': serializable_results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description='æ‹¡å¼µå¯èƒ½ãªå®Ÿé¨“å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--patterns', nargs='+', required=True, 
                       help='å®Ÿè¡Œã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒªã‚¹ãƒˆ (ä¾‹: chat_abs_gen chat_strict_gen)')
    parser.add_argument('--runs', type=int, default=1, 
                       help='å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Ÿè¡Œå›æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1)')
    parser.add_argument('--language', default='ja', 
                       help='è¨€èª (ja/en, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ja)')
    parser.add_argument('--output-dir', 
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆ)')
    
    args = parser.parse_args()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ±ºå®š
    if args.output_dir:
        base_output_dir = args.output_dir
    else:
        timestamp = datetime.now().strftime("%Y%m%d%H%M")
        base_output_dir = f"test_logs/{timestamp}_multi_experiments"
    
    print("ğŸš€ æ‹¡å¼µå¯èƒ½ãªå®Ÿé¨“å®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"ğŸ“‹ ãƒ‘ã‚¿ãƒ¼ãƒ³: {', '.join(args.patterns)}")
    print(f"ğŸ”„ å®Ÿè¡Œå›æ•°: {args.runs}å›/ãƒ‘ã‚¿ãƒ¼ãƒ³")
    print(f"ğŸŒ è¨€èª: {args.language}")
    print(f"ğŸ“ å‡ºåŠ›å…ˆ: {base_output_dir}")
    print()
    
    # å®Ÿé¨“è¨­å®šã‚’ä½œæˆ
    configs = []
    for pattern in args.patterns:
        config = ExperimentConfig(pattern=pattern, language=args.language, runs=args.runs)
        configs.append(config)
    
    # å®Ÿé¨“å®Ÿè¡Œ
    runner = ExperimentRunner(base_output_dir)
    runner.run_experiments(configs)
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
    print("\nğŸ“Š ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†ä¸­...")
    log_data = runner.collect_log_files()
    
    # çµ±è¨ˆã‚’è¨ˆç®—
    stats = runner.generate_statistics(log_data)
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    runner.generate_report(stats)
    
    # çµæœã‚’ä¿å­˜
    runner.save_results(stats, log_data)
    
    print(f"\nâœ… å®Ÿé¨“å®Œäº†: {base_output_dir}")

if __name__ == "__main__":
    main()
